# CS653 Final Exam

# ‡∏Ç‡πâ‡∏≠ 1

- **‡∏™‡∏£‡πâ‡∏≤‡∏á S3 bucket:** ‡∏™‡∏£‡πâ‡∏≤‡∏á S3 bucket ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡πÄ‡∏ä‡πà‡∏ô `studentxxxx-yyyy-customers`) ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏∑‡πà‡∏≠ `csv/` ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô bucket ‡∏ô‡∏±‡πâ‡∏ô.
- **‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå:** ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `customers.csv` ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `csv/` ‡πÉ‡∏ô S3 bucket.
- **‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Database:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Database ‡πÉ‡∏ô Glue Data Catalog ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î (‡πÄ‡∏ä‡πà‡∏ô `customers_xxxx_yyyy`).
- **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ Glue Crawler:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Crawler ‡πÇ‡∏î‡∏¢‡∏ä‡∏µ‡πâ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `csv/` ‡πÉ‡∏ô S3 bucket **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ IAM role ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà (`labrole`)** ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metadata ‡∏•‡∏á‡πÉ‡∏ô Glue Database ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ.
- **‡∏£‡∏±‡∏ô Glue Crawler:** ‡∏£‡∏±‡∏ô Crawler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Glue ‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô S3 ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Table Definition ‡πÉ‡∏ô Data Catalog.
- **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Table:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Table ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô Glue Database ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏ô Glue Data Catalog ‡πÅ‡∏•‡πâ‡∏ß (‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô `csv`).
- **‡πÉ‡∏ä‡πâ Athena ‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Amazon Athena ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Database ‡πÅ‡∏•‡∏∞ Table ‡∏ó‡∏µ‡πà Glue ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á SQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å 'Thailand' ‡πÄ‡∏ä‡πà‡∏ô `SELECT * FROM "‡∏ä‡∏∑‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"."‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏£‡∏≤‡∏á" WHERE country = 'Thailand';`

# ‡∏Ç‡πâ‡∏≠ 2

- **‡∏™‡∏£‡πâ‡∏≤‡∏á S3 bucket ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå:** ‡∏™‡∏£‡πâ‡∏≤‡∏á S3 bucket ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô `studentxxxx-yyyy-products`) ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏∑‡πà‡∏≠ `csv/` ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô bucket ‡∏ô‡∏±‡πâ‡∏ô.
- **‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå:** ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `products.csv` ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `csv/` ‡πÉ‡∏ô S3 bucket ‡πÉ‡∏´‡∏°‡πà.
- **‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Database ‡πÉ‡∏´‡∏°‡πà:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Database ‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏ô Glue Data Catalog ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô `products_xxxx_yyyy`).
- **‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏ã‡∏• Athena ‡∏™‡∏£‡πâ‡∏≤‡∏á Table ‡∏à‡∏≤‡∏Å S3:** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Amazon Athena ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏ô‡πÇ‡∏ã‡∏• ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå **"Create"** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **"Create a table from data source"** ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **"S3 bucket data"**. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤ **Database** ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ **Table** (‡πÄ‡∏ä‡πà‡∏ô `test`). ‡∏Å‡∏≥‡∏´‡∏ô‡∏î **Location** ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `csv/` ‡πÉ‡∏ô S3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **Format** ‡πÄ‡∏õ‡πá‡∏ô **CSV**. ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Schema (Column Details) ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì (id bigint, name string, email string, category string, signup_date string).
- **‡πÉ‡∏ä‡πâ Athena ‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡πÉ‡∏ô Athena ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á SQL ‡∏Ñ‡∏¥‡∏ß‡∏£‡∏µ‡∏ô‡∏µ‡πâ‡∏à‡∏≤‡∏Å‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ (‡πÄ‡∏ä‡πà‡∏ô `test`) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏õ‡∏µ 2023

```sql
SELECT
    category,
    COUNT(*) AS interested_users_count
FROM
    "products_6120_3987"."test"
WHERE
    TRY_CAST(id AS BIGINT) IS NOT NULL
    AND CAST(substr(signup_date, length(signup_date) - 3) AS INTEGER) = 2023
GROUP BY
    category
ORDER BY
    interested_users_count DESC
LIMIT 1;
```

# ‡∏Ç‡πâ‡∏≠ 3

- **‡∏™‡∏£‡πâ‡∏≤‡∏á SNS Topic ‡πÅ‡∏•‡∏∞‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡∏™‡∏£‡πâ‡∏≤‡∏á SNS Topic ‡πÅ‡∏ö‡∏ö **Standard** (‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô `temperature-alerts-xxxx-yyyy`) ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö **Email subscription** ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì ‡πÅ‡∏•‡∏∞ **‡∏Å‡∏î‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô (Confirm)** ‡πÉ‡∏ô‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å AWS.
- **‡∏™‡∏£‡πâ‡∏≤‡∏á DynamoDB Table:** ‡∏™‡∏£‡πâ‡∏≤‡∏á DynamoDB Table (‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô `iot-temperature-data-xxxx-yyyy`) ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î **Partition Key** ‡πÄ‡∏õ‡πá‡∏ô **`roomid`** ‡∏ä‡∏ô‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• **String**.
- **‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda Function:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Lambda Function (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Runtime ‡πÄ‡∏õ‡πá‡∏ô **Python**) ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô `iot-temperature-xxxx-yyyy-logs`. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ **`labrole`** ‡πÄ‡∏õ‡πá‡∏ô Execution Role. (‡∏ó‡∏£‡∏≤‡∏ö‡∏ß‡πà‡∏≤ Role ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡πâ‡∏ß).
- **‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Code ‡πÉ‡∏ô Lambda Function:** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö **Code** ‡∏Ç‡∏≠‡∏á Lambda Function. ‡∏•‡∏ö Code ‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏≠‡∏Å ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏≤‡∏á Code ‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÑ‡∏õ:
    
    ```sql
    import json
    import boto3
    import os
    from decimal import Decimal
    
    # Initialize AWS clients
    dynamodb = boto3.resource('dynamodb')
    sns = boto3.client('sns')
    
    # --- REPLACE THESE WITH YOUR ACTUAL RESOURCE NAMES/ARNS ---
    # Replace 'arn:aws:sns:us-east-1:304816568247:temperature-alerts-6120-3987'
    # with the ACTUAL ARN of your SNS Topic. Find this in the SNS console.
    sns_topic_arn = 'arn:aws:sns:us-east-1:304816568247:temperature-alerts-6120-3987'
    
    # Replace 'iot-temperature-data-6120-3987'
    # with the ACTUAL name of your DynamoDB Table. Find this in the DynamoDB console.
    dynamodb_table_name = 'iot-temperature-data-6120-3987'
    # --- END OF REPLACEMENT SECTION ---
    
    table = dynamodb.Table(dynamodb_table_name)
    
    def lambda_handler(event, context):
        print("Received event: " + json.dumps(event))
    
        try:
            # Handle cases where the event might be a stringified JSON or a dictionary
            message = event
            if isinstance(event, str):
                 message = json.loads(event)
    
            room_id = message.get('roomid')
            timestamp = message.get('timestamp')
            temperature_raw = message.get('temperature')
    
            if room_id is None or timestamp is None or temperature_raw is None:
                print("Missing required fields in message")
                # Optionally send an SNS alert for invalid data structure
                try:
                    sns.publish(
                        TopicArn=sns_topic_arn,
                        Subject='IoT Temperature Alert - Missing Fields',
                        Message=f'Received message with missing fields: {json.dumps(message)}'
                    )
                except Exception as sns_error:
                     print(f"Failed to send SNS alert for missing fields: {sns_error}")
    
                return {
                    'statusCode': 400,
                    'body': json.dumps('Missing required fields')
                }
    
            # Convert temperature to Decimal for DynamoDB and float for comparison
            try:
                # Convert raw temperature value to string first for robust Decimal conversion
                temperature_decimal = Decimal(str(temperature_raw))
                temperature_float = float(temperature_decimal) # Keep float version for comparison
            except Exception as e: # Catch errors during conversion
                print(f"Invalid temperature value received: {temperature_raw}. Error: {e}")
                # Send SNS alert for invalid data value
                try:
                    sns.publish(
                        TopicArn=sns_topic_arn,
                        Subject='IoT Temperature Alert - Invalid Data Value',
                        Message=f'Received invalid temperature data value: {temperature_raw} for Room ID: {room_id}. Error: {e}'
                    )
                except Exception as sns_error:
                     print(f"Failed to send SNS alert for invalid value: {sns_error}")
    
                return {
                    'statusCode': 400,
                    'body': json.dumps('Invalid temperature value')
                }
    
            # --- Save data to DynamoDB ---
            # Use Decimal type for temperature when putting into DynamoDB
            table.put_item(
                Item={
                    'roomid': str(room_id),
                    'timestamp': str(timestamp),
                    'temperature': temperature_decimal
                }
            )
            print(f"Successfully saved data to DynamoDB for room: {room_id}, temp: {temperature_float}")
    
            # --- Check temperature and send SNS alert if out of range ---
            if temperature_float < 2.0 or temperature_float > 8.0:
                alert_message = f"Temperature out of range! Room ID: {room_id}, Temperature: {temperature_float}¬∞C at {timestamp}"
                print(f"Temperature out of range: {alert_message}")
    
                # Publish alert to SNS
                try:
                    sns.publish(
                        TopicArn=sns_topic_arn,
                        Subject='IoT Temperature Alert - Out of Range',
                        Message=alert_message
                    )
                    print("SNS out-of-range alert sent.")
                except Exception as sns_error:
                     print(f"Failed to send SNS out-of-range alert: {sns_error}")
    
            else:
                print(f"Temperature within range: {temperature_float}¬∞C")
    
            return {
                'statusCode': 200,
                'body': json.dumps('Processing complete')
            }
    
        except Exception as e:
            # Catch any other unexpected errors
            print(f"An unexpected error occurred during event processing: {e}")
            # Send general SNS alert for processing errors
            try:
                sns.publish(
                     TopicArn=sns_topic_arn,
                     Subject='IoT Temperature Alert - Unexpected Lambda Error',
                     Message=f'Unexpected error processing IoT message: {json.dumps(event)}. Error: {e}'
                )
                print("SNS unexpected error alert sent.")
            except Exception as sns_error:
                print(f"Failed to send SNS unexpected error alert: {sns_error}")
    
            return {
                'statusCode': 500,
                'body': json.dumps(f'Error processing event: {e}')
            }
    ```
    
- **‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡πÉ‡∏ô Code ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô ‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡πà‡∏≤ **`sns_topic_arn`** ‡πÅ‡∏•‡∏∞ **`dynamodb_table_name`** ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô **ARN ‡∏Ç‡∏≠‡∏á SNS Topic ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠ DynamoDB Table ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ** ‡πÇ‡∏î‡∏¢‡∏î‡∏π‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ console ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡πâ‡∏ô‡πÜ. ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ï‡πâ Comment
- **‡∏™‡∏£‡πâ‡∏≤‡∏á IoT Rule:** ‡πÑ‡∏õ‡∏ó‡∏µ‡πà AWS IoT Core Console > **Message routing** > **Rules**. ‡∏™‡∏£‡πâ‡∏≤‡∏á Rule ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡πÄ‡∏ä‡πà‡∏ô `IotRule_coldroomxxxx_yyyy`. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î **SQL statement** ‡πÄ‡∏õ‡πá‡∏ô `SELECT * FROM 'coldroomxxxx-yyyy/temperature'` (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà xxxx-yyyy). ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô **Rule actions** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å **Send a message to a Lambda function** ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Lambda Function ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì. (‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Invoke Lambda ‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÇ‡∏î‡∏¢ Lab Role ‡πÅ‡∏•‡πâ‡∏ß).
- **‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ MQTT Test Client:**
    - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà AWS IoT Core > **MQTT test client**.
    - ‡∏Å‡∏î **Connect** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠.
    - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö **Subscribe to a topic** ‡πÅ‡∏•‡∏∞‡∏û‡∏¥‡∏°‡∏û‡πå **`coldroomxxxx-yyyy/temperature`** ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î **Subscribe** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà Publish ‡πÑ‡∏õ.
    - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö **Publish to a topic**. ‡∏û‡∏¥‡∏°‡∏û‡πå **Topic name** ‡πÄ‡∏õ‡πá‡∏ô **`coldroomxxxx-yyyy/temperature`**.
    - ‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á **Message payload** ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥‡πÉ‡∏ô **Format ‡πÅ‡∏ö‡∏ö JSON ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö (‡πÑ‡∏°‡πà‡∏°‡∏µ Newline ‡∏´‡∏£‡∏∑‡∏≠ Indent ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ)** ‡πÄ‡∏ä‡πà‡∏ô **`{"roomid":"R001","timestamp":"2025-05-07T19:15:00Z","temperature":5.5}`**.
    - ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Publish ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° **10 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á** ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥ **‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 2¬∞C - 8¬∞C 3 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á** ‡πÅ‡∏•‡∏∞ **‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á 2¬∞C - 8¬∞C  7 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á**.

# ‡∏Ç‡πâ‡∏≠ 4

- ‡∏™‡∏£‡πâ‡∏≤‡∏á Cloud9 Environment ‡∏ä‡∏∑‡πà‡∏≠ `feed-temperature6120-3987` ‡πÇ‡∏î‡∏¢‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Instance ‡πÅ‡∏ö‡∏ö `t2.micro`, ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ Amazon Linux 2023 ‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡πÅ‡∏ö‡∏ö **SSH** ‡πÅ‡∏ó‡∏ô SSM (‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏±‡∏ç‡∏ä‡∏µ AWS Academy ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏™‡∏£‡πâ‡∏≤‡∏á IAM Role ‡πÄ‡∏≠‡∏á) ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏£‡∏≠‡πÉ‡∏´‡πâ Environment ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ Terminal
- ‡∏™‡∏£‡πâ‡∏≤‡∏á S3 Bucket ‡∏ä‡∏∑‡πà‡∏≠ `s3-temperature-store-6120-3987` ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Region ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Cloud9 ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î public access
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Amazon Data Firehose Stream ‡∏ä‡∏∑‡πà‡∏≠ `temperature-stream-6120-3987` ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Source ‡πÄ‡∏õ‡πá‡∏ô **Direct PUT**, Destination ‡πÄ‡∏õ‡πá‡∏ô **Amazon S3**, ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Bucket ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡∏Ñ‡∏∑‡∏≠ `s3-temperature-store-6120-3987`, ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å IAM Role ‡πÄ‡∏õ‡πá‡∏ô `labRole` ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÉ‡∏î (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏¥‡∏î data transformation, format conversion, ‡∏´‡∏£‡∏∑‡∏≠ decompression)
- ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Cloud9 ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠ `send_temp.py` ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏™‡πà‡πÇ‡∏Ñ‡πâ‡∏î‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏•‡∏á‡πÑ‡∏õ:
    
    ```sql
    import boto3
    import json
    import time
    import random
    
    firehose = boto3.client('firehose', region_name='us-east-1') -- ‡∏£‡∏∞‡∏ß‡∏±‡∏á Region
    stream_name = 'temperature-stream-6120-3987'
    
    while True:
        temperature_data = {
            'temperature': round(random.uniform(25.0, 40.0), 2),
            'unit': 'C',
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        response = firehose.put_record(
            DeliveryStreamName=stream_name,
            Record={
                'Data': json.dumps(temperature_data) + '\n'
            }
        )
        print(f"Sent: {temperature_data}")
        time.sleep(3)
    
    ```
    

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `python3 send_temp.py` ‡πÉ‡∏ô Terminal ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏ô‡∏≤‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡πÉ‡∏´‡πâ Firehose ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ buffer ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ S3

- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö S3 Bucket ‡∏ó‡∏µ‡πà `s3-temperature-store-6120-3987` ‡∏à‡∏∞‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå `.json.gz` ‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏†‡∏≤‡∏¢‡πÉ‡∏ô path ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ Firehose ‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô `/2025/05/08/06/` ‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 7KB ‡∏ã‡∏∂‡πà‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å Python script

# ‡∏Ç‡πâ‡∏≠ 5

- ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `feedbacks.csv` ‡πÑ‡∏õ‡∏¢‡∏±‡∏á S3 ‡∏ó‡∏µ‡πà path: `s3://student6120-3987-ml/raw/`
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á S3 Bucket
    - ‡∏™‡∏£‡πâ‡∏≤‡∏á Folder ‡∏ä‡∏∑‡πà‡∏≠ raw
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Job > Visual ETL > ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö Job Details ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà Code ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö Script

| ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
| --- | --- |
| **Job name** | `csv_to_json_preprocess` ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ |
| **IAM Role** | `AWSGlueServiceRoleDefault` ‡∏´‡∏£‡∏∑‡∏≠ `LabRole` (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô S3) |

```python
import sys
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

df = spark.read.option("header", "true").csv("s3://student6120-3987-ml/raw/feedbacks.csv")
df.write.mode("overwrite").json("s3://student6120-3987-ml/preprocessed/")

job.commit()
```

- ‡∏™‡∏£‡πâ‡∏≤‡∏á Sentiment Endpoint ‡∏î‡πâ‡∏ß‡∏¢ CloudFormation
    - Deploy Template ‡∏î‡πâ‡∏ß‡∏¢ `sentiment.yaml` ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÑ‡∏ß‡πâ
    - ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠ Stack ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô `sentiment-api`
    - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å VPC
    - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å LabRole ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    - ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à ‚Üí ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö `Outputs` ‚Üí ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤ `SentimentEndpoint` ‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ `/predict`
    ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á endpoint > [`http://ec2-44-201-210-179.compute-1.amazonaws.com/predict`](http://ec2-44-201-210-179.compute-1.amazonaws.com/predict)
- ‡πÅ‡∏Å‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô Glue Job > save > run

```python
import sys
import json
import requests
import pandas as pd
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import DoubleType, StringType

# üîß Replace with your deployed endpoint
SENTIMENT_ENDPOINT = "http://ec2-44-201-210-179.compute-1.amazonaws.com/predict" --‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ‚úÖ Load preprocessed JSON (not CSV!) from S3
input_df = spark.read.json("s3://student6120-3987-ml/preprocessed/")

# ‚úÖ Define sentiment analysis function with error key always included
def analyze_sentiment(row):
    base = {
        "text": row['text'],
        "sentiment": None,
        "score_positive": None,
        "score_negative": None,
        "score_neutral": None,
        "error": None
    }
    try:
        response = requests.post(
            SENTIMENT_ENDPOINT,
            json={"text": row['text']},
            timeout=5
        )
        result = response.json()
        base.update({
            "sentiment": result.get("sentiment"),
            "score_positive": result.get("scores", {}).get("positive"),
            "score_negative": result.get("scores", {}).get("negative"),
            "score_neutral": result.get("scores", {}).get("neutral")
        })
    except Exception as e:
        base["sentiment"] = "ERROR"
        base["error"] = str(e)
    return base

# ‚úÖ UDF wrapper
@pandas_udf("text string, sentiment string, score_positive double, score_negative double, score_neutral double, error string")
def predict_udf(text_series: pd.Series) -> pd.DataFrame:
    results = []
    for text in text_series:
        row = {"text": text}
        results.append(analyze_sentiment(row))
    return pd.DataFrame(results)

# ‚úÖ Process and write output
output_df = input_df.select("text")
final_df = output_df.withColumn("result", predict_udf("text")).selectExpr("result.*")
final_df.write.mode("overwrite").json("s3://student6120-3987-ml/processed/")

job.commit()

```

- ‡πÄ‡∏Ç‡πâ‡∏≤ Athena ‡πÅ‡∏•‡πâ‡∏ß Run SQL 2 ‡∏≠‡∏±‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS sentiment_results (
  text STRING,
  sentiment STRING,
  score_positive DOUBLE,
  score_negative DOUBLE,
  score_neutral DOUBLE,
  error STRING
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
LOCATION 's3://student6120-3987-ml/processed/';
```

```sql
SELECT sentiment, COUNT(*) AS total
FROM sentiment_results
WHERE sentiment IS NOT NULL
GROUP BY sentiment;
```

# ‡∏Ç‡πâ‡∏≠  6

- ‡∏™‡∏£‡πâ‡∏≤‡∏á Bucket `s3://studentxxxx-yyyy-ml/` > ‡∏™‡∏£‡πâ‡∏≤‡∏á Folder raw
- ‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡∏¢‡∏±‡∏á S3 ‡∏ó‡∏µ‡πà path: `s3://studentxxxx-yyyy-ml/raw/`
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Glue Job > Visual ETL > ‡∏Å‡∏£‡∏≠‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≤‡∏á‡πÜ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö Job Details ‡πÅ‡∏•‡∏∞‡πÉ‡∏™‡πà Code ‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ö Script > run

```python
import sys
import boto3
import json
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î bucket ‡πÅ‡∏•‡∏∞ prefix ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û
bucket = "student6120-3987-ml"
prefix = "raw/"

# ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô S3
s3 = boto3.client('s3')
response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞ URI
files = [
    {
        "filename": obj["Key"].split("/")[-1],
        "s3uri": f"s3://{bucket}/{obj['Key']}"
    }
    for obj in response.get("Contents", [])
    if obj["Key"].endswith(".jpg")
]

# ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡∏•‡∏á S3
rdd = sc.parallelize(files)
df = spark.read.json(rdd)
df.write.mode("overwrite").json("s3://student6120-3987-ml/preprocessed/")

job.commit()

```

- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô code Glue Job ‡πÄ‡∏õ‡πá‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á > save > run

```python
import sys
import json
import boto3
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.types import StructType, StructField, StringType, BooleanType, DoubleType

# Step 1: Glue job setup
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Step 2: Load input JSON from preprocessed path
input_df = spark.read.json("s3://student6120-3987-ml/preprocessed/")

# Step 3: Define output schema explicitly
schema = StructType([
    StructField("filename", StringType(), True),
    StructField("has_face", BooleanType(), True),
    StructField("gender", StringType(), True),
    StructField("confidence", DoubleType(), True),
    StructField("error", StringType(), True)
])

# Step 4: Detect faces using AWS Rekognition
def detect_face(row):
    result = {
        "filename": row["filename"],
        "has_face": False,
        "gender": None,
        "confidence": None,
        "error": None
    }
    try:
        rekognition = boto3.client('rekognition')
        response = rekognition.detect_faces(
            Image={"S3Object": {
                "Bucket": "student6120-3987-ml",
                "Name": f"raw/{row['filename']}"
            }},
            Attributes=["ALL"]
        )
        if response["FaceDetails"]:
            face = response["FaceDetails"][0]
            result["has_face"] = True
            result["gender"] = face["Gender"]["Value"]
            result["confidence"] = face["Gender"]["Confidence"]
    except Exception as e:
        result["error"] = f"{str(e)} | file={row['filename']}"  # ‚úÖ more traceable

    return result

# Step 5: Apply and write results to processed/
rows_rdd = input_df.rdd.map(lambda row: detect_face(row.asDict()))
output_df = spark.createDataFrame(rows_rdd, schema=schema)
output_df.write.mode("overwrite").json("s3://student6120-3987-ml/processed/")

job.commit()

```

- ‡∏™‡∏£‡πâ‡∏≤‡∏á Table ‡πÉ‡∏ô AWS Athena

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS processed_faces (
  filename   string,
  has_face   boolean,
  gender     string,
  confidence double,
  error      string
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
LOCATION 's3://student6120-3987-ml/processed/'
TBLPROPERTIES ('has_encrypted_data'='false');

```

- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤

```sql
SELECT COUNT(*) AS face_detected_count
FROM processed_faces
WHERE has_face = true;
```

- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏û‡∏®‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö (‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤)

```sql
SELECT gender, COUNT(*) AS count
FROM processed_faces
WHERE has_face = true
GROUP BY gender;
```

- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤

```sql
SELECT COUNT(*) AS no_face_count
FROM processed_faces
WHERE has_face = false;
```
